---
title: "incomePred"
---

# Load Libraries

```{r load_libraries}
library(janitor)
library(bayesrules)
library(rstanarm)
library(tidyverse)
library(bayesplot)
```

# Literature Review

Lecture Review:
1994 Census Income dataset.

Marcel Jirina and Marcel Jirina Jr.
Inverted Index of Neighbor Classifier (IINC), an enhanced classification method designed to improve upon traditional nearest neighbor classifiers. Optimizing the distance function used to measure similarity between data points. The choice of distance function significantly impacts the classification error rate.

Mathieu Guillame-Bert and Artur Dubrawski
Batched Lazy Decision Tress utilizes an algorithm that avoids unnecessary visits to irrelevant nodes when it is used to make predictions with either eagerly or lazily trained decision trees. Improved computation time as well as memory consumption without compromising accuracy.

Ron Kohhavi
NBTree, a hybrid of decision-tree classifiers and Naïve-Bayes classifiers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naïve Bayes classifiers. Keeps the interpretability of Naïve-Bayes and decision trees but is scalable. 


# Reading in Data

```{r read}
all_data <- read.csv("adult.data")
colnames(all_data) <- c("age", "workclass", "final_weight","education", "education_num", "martial-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours_per_week", "native-country", "income")
```

# Project Objective

Given information about an individual adult in the U.S. and we want to use Bayesian logistic regression to classify whether or not they earn more 50k or not. Specifically, we will look at the number of years of education, sex, race, and age.

# Exploratory Data Analysis

```{r clean}
adult_income <- all_data[, c("education_num", "sex", "race", "age", "hours_per_week", "income")]

adult_income$sex <- trimws(adult_income$sex)
adult_income$sex <- ifelse(adult_income$sex == "Male", 1, 0)

adult_income$income <- trimws(adult_income$income)
adult_income$income <- ifelse(adult_income$income == ">50K", 1, 0)

adult_income$race <- trimws(adult_income$race)
adult_income$race <- ifelse(adult_income$race == "White", 0,
                     ifelse(adult_income$race == "Black", 1,
                     ifelse(adult_income$race == "Asian-Pac-Islander", 2,
                     ifelse(adult_income$race == "Amer-Indian-Eskimo", 3, 4))))

colSums(is.na(adult_income))  # Count missing values in each column
sum(is.na(adult_income))  # Total missing values in dataset
```

```{r exploratory}

table(adult_income$race)  # Frequency count of Race
table(adult_income$sex)  # Frequency count of Sex 
table(adult_income$income)  # Frequency count of Income 

barplot(table(adult_income$race), main="Bar Chart of Race", col="lightblue", names.arg = c("White", "Black", "Asian", "Eskimo", "Other"), xlab = "Race", ylab = "Frequency")

barplot(table(adult_income$sex), main="Bar Chart of Sex", col="lightblue", names.arg = c("Female", "Male"), xlab = "Sex", ylab = "Frequency")

barplot(table(adult_income$income), main="Bar Chart of Income", col="lightblue", names.arg = c("<=50K", ">50K"), xlab = "Income", ylab = "Frequency")


hist(adult_income$age, main="Histogram of Age", col="lightblue", breaks=25, xlab = "Age", ylab = "Frequency")
boxplot(adult_income$age, main="Boxplot of Age", col="lightblue", horizontal = TRUE, xlab = "Age")

hist(adult_income$education_num, main="Histogram of Years of Education", col="lightblue", breaks=5, xlab = "Years of Education", ylab = "Frequency")
boxplot(adult_income$education_num, main="Boxplot of Years of Education", col="lightblue", horizontal = TRUE, xlab = "Years of Education")

hist(adult_income$hours_per_week, main="Histogram of Hours per Week", col="lightblue", breaks=25, xlab = "Hours per Week", ylab = "Frequency")
boxplot(adult_income$hours_per_week, main="Boxplot of Hours per Week", col="lightblue", horizontal = TRUE, xlab = "Hours per Week")
```

# Simple Model that uses just the Years of Education to Predict Income Classifications

## Model Specifications
```{r simple_income_model}
set.seed(84735)  # For reproducibility
train_indices <- sample(seq_len(nrow(adult_income)), size = 0.8 * nrow(adult_income))

train_data <- adult_income[train_indices,]
test_data <- adult_income[-train_indices,]

income_model_1 <- stan_glm(income ~ education_num,
                           data = train_data,
                           family = binomial,
                           chains = 4,
                           iter = 5000*2,
                           seed = 84735)
prior_summary(income_model_1)
mcmc_dens_overlay(income_model_1)
mcmc_trace(income_model_1)

income_model_1_df <- as.data.frame(income_model_1)

first_50 <- head(income_model_1_df, 50)

prob_trend <- function(beta0, beta1, x){
  exp(beta0 + beta1*x) / (1 + exp(beta0 + beta1*x))
}

ggplot(train_data, aes(x=education_num, y=income)) +
  mapply(function(beta0, beta1){
    stat_function(fun = prob_trend,
                  args = list(beta0 = beta0, beta1 = beta1), linewidth = 0.1)
  },
  beta0 = first_50$`(Intercept)`, beta1 = first_50$education_num) +
  labs(y = "probability of making more than 50K")
``` 

## Predictions
```{r model_1_pred}
# tr_income_pred_1 <- posterior_predict(income_model_1, newdata = train_data)
te_income_pred_1 <- posterior_predict(income_model_1, newdata = test_data)

# tr_income_classifications <- train_data %>%
  # mutate(income_prop = colMeans(tr_income_pred_1)) %>%
  # mutate(income_class_1 = as.numeric(income_prop >= 0.5)) %>%
  # select(education_num, income_prop, income_class_1, income)


te_income_classifications <- test_data %>%
  mutate(income_prop = colMeans(te_income_pred_1)) %>%
  mutate(income_class_1 = as.numeric(income_prop >= 0.5)) %>%
  select(education_num, income_prop, income_class_1, income)

# tabyl(tr_income_classifications, income, income_class_1)
# classification_summary(model = income_model_1, data = train_data, cutoff = 0.5)

tabyl(te_income_classifications, income, income_class_1)
classification_summary(model = income_model_1, data = test_data, cutoff = 0.5)
```

# Simple Model that uses just Hours Per Week to Predict Income Classifications
## Model Specifications

```{r model_specs}
income_model_hours_per_week <- stan_glm(income ~ hours_per_week,
                           data = train_data,
                           family = binomial,
                           chains = 4,
                           iter = 5000*2,
                           seed = 84735)
prior_summary(income_model_hours_per_week)
mcmc_dens_overlay(income_model_hours_per_week)
mcmc_trace(income_model_hours_per_week)

income_model_hours_per_week_df <- as.data.frame(income_model_hours_per_week)

first_50 <- head(income_model_hours_per_week_df, 50)

prob_trend <- function(beta0, beta1, x){
  exp(beta0 + beta1*x) / (1 + exp(beta0 + beta1*x))
}

ggplot(train_data, aes(x=hours_per_week, y=income)) +
  mapply(function(beta0, beta1){
    stat_function(fun = prob_trend,
                  args = list(beta0 = beta0, beta1 = beta1), linewidth = 0.1)
  },
  beta0 = first_50$`(Intercept)`, beta1 = first_50$hours_per_week) +
  labs(y = "probability of making more than 50K")
``` 

# Simple Model that uses just Age to Predict Income Classifications
## Model Specifications

```{r model_specs}
income_model_age <- stan_glm(income ~ age,
                           data = train_data,
                           family = binomial,
                           chains = 4,
                           iter = 5000*2,
                           seed = 84735)
prior_summary(income_model_age)
mcmc_dens_overlay(income_model_age)
mcmc_trace(income_model_age)

income_model_age_df <- as.data.frame(income_model_age)

first_50 <- head(income_model_age_df, 50)

prob_trend <- function(beta0, beta1, x){
  exp(beta0 + beta1*x) / (1 + exp(beta0 + beta1*x))
}

ggplot(train_data, aes(x=age, y=income)) +
  mapply(function(beta0, beta1){
    stat_function(fun = prob_trend,
                  args = list(beta0 = beta0, beta1 = beta1), linewidth = 0.1)
  },
  beta0 = first_50$`(Intercept)`, beta1 = first_50$age) +
  labs(y = "probability of making more than 50K")
```

# Model 1 
## Years of Education and Age
```{r model 2}
income_model_1 <- stan_glm(income ~ education_num + age,
                           data = train_data,
                           family = binomial,
                           chains = 4,
                           iter = 5000*2,
                           seed = 84735)

prior_summary(income_model_1)
mcmc_dens_overlay(income_model_1)
mcmc_trace(income_model_1)
```

## Predictions

```{r model_1_pred}
te_income_pred_1 <- posterior_predict(income_model_1, newdata = test_data)

te_income_classifications_1 <- test_data %>%
  mutate(income_prop = colMeans(te_income_pred_1)) %>%
  mutate(income_class_1 = as.numeric(income_prop >= 0.5)) %>%
  select(education_num, income_prop, income_class_1, income)

tabyl(te_income_classifications_1, income, income_class_1)
classification_summary(model = income_model_1, data = test_data, cutoff = 0.5)
```

# Model 2
## Years of Education, Age, and Sex

```{r model_2}

income_model_2 <- stan_glm(income ~ education_num + age + factor(sex),
                           data = train_data,
                           family = binomial,
                           chains = 4,
                           iter = 5000*2,
                           seed = 84735)

prior_summary(income_model_2)
mcmc_dens_overlay(income_model_2)
mcmc_trace(income_model_2)

```

## Predictions
```{r model_2_pred}
te_income_pred_2 <- posterior_predict(income_model_2, newdata = test_data)

te_income_classifications_2 <- test_data %>%
  mutate(income_prop = colMeans(te_income_pred_2)) %>%
  mutate(income_class_1 = as.numeric(income_prop >= 0.5)) %>%
  select(education_num, income_prop, income_class_1, income)

tabyl(te_income_classifications_2, income, income_class_1)
classification_summary(model = income_model_2, data = test_data, cutoff = 0.5)
```

# Model 3
## Years of Education, Age, Sex, and Hours Per Week

```{r model_3}
income_model_3 <- stan_glm(income ~ education_num + age + factor(sex) + hours_per_week,
                           data = train_data,
                           family = binomial,
                           chains = 4,
                           iter = 5000*2,
                           seed = 84735)

prior_summary(income_model_3)
mcmc_dens_overlay(income_model_3)
mcmc_trace(income_model_3)
```

## Predictions
```{r model_3_pred}
te_income_pred_3 <- posterior_predict(income_model_3, newdata = test_data)

te_income_classifications_3 <- test_data %>%
  mutate(income_prop = colMeans(te_income_pred_3)) %>%
  mutate(income_class_1 = as.numeric(income_prop >= 0.5)) %>%
  select(education_num, income_prop, income_class_1, income)

tabyl(te_income_classifications_3, income, income_class_1)
classification_summary(model = income_model_3, data = test_data, cutoff = 0.5)
```

# Prior Intuition

So for our logistic regression, the prior should reflect reasonable assumptions about the relationship between predictors and the log-odds (or odds ratio) of the outcome. For years of education, education_num, the coefficient should represent the expected change in logs-odd for every 1-unit (1 year) increase in education. We assumed a prior of 0.10 would be reasonable. It implies there's an approximate 9% increase in odds per unit increase (1 year of additional education), indicating there's a strong association between the years of education and whether or not they make more than 50K. For sex, since its a categorical and binary variable, it would either be 0 or 1. This would mean that the our coefficient for sex would be the difference in log-odds from going 0 (female) to 1 (male) while keeping all other factors constant. Given that this data set was taken from 1994, we would assume that the pay gap between males and females existed more prominently. We assumed a prior of 0.5 would be reasonable. It implies there's an approximate 33% increase in odds going from female to male, indicating there's also a strong association between sex and whether or not they make more than 50K. For race, there's a total of 4 different coefficients, each representing the difference in log-odds from the baseline (white). For the coefficient of each race, we assumed that there will be a decrease in the log-odds for every race when comparing to the baseline. In particular, we did -0.5 all. As for age, we believe there's that income tends to increase with age but at a diminishing rate. So, We assume a small positive effect of 0.02 or 2% increase in log-odds per increase in age. As for the standard deviation of the priors, we chose 0.1 for ones that we were less confident in so there's room for flexibility. For example, the number of years of education, A PhD versus a high school diploma has a huge difference, but extra years of education might not always translate to higher earnings. So allow for more variation in `education_num`. However for age, since age effects on income are well-documented and we expect a consistent but small positive relationship, we chose 0.01 to reflect our confidence. We did the same for race and sex. Since we were confident in the sex one, we left less room for variation, hence why we chose 0.1. For race, since we weren't sure if the change was the same for each race, we left more room for variation, hence the 0.1.

# Key Takeaways

The model performs worse when we include race. Even though based on prior knowledge we know that race plays a factor in determining how much an individual makes, the data too unbalanced to the point where the effects of race are masked. Add


















# Ignore this
## Testing Zone

```{r old code}
ggplot(train_data, aes(x = education_num, y = income)) + mapply(function(beta0, beta_edu, beta_sex, beta_race1, beta_race2, beta_race3, beta_race4, beta_age) {
  stat_function(fun = prob_trend,
                args = list(beta0 = beta0,
                            beta_edu = beta_edu,
                            beta_sex = beta_sex,
                            beta_race1 = beta_race1,
                            beta_race2 = beta_race2,
                            beta_race3 = beta_race3,
                            beta_race4 = beta_race4,
                            beta_age = beta_age), size = 0.1)
},
beta0 = first_50$`(Intercept)`, 
beta_edu = first_50$education_num,
beta_sex = first_50$`factor(sex)1`,
beta_race1 = first_50$`factor(race)1`,
beta_race2 = first_50$`factor(race)2`,
beta_race3 = first_50$`factor(race)3`,
beta_race4 = first_50$`factor(race)4`,
beta_age = first_50$age
)

income_model_2_df <- as.data.frame(income_model_2)




predictions <- first_50 %>%
  mutate(id = row_number()) %>%
  rowwise() %>%
  mutate(prob = mean(map_dbl(1:50, function(i) {
    prob_trend(
      beta0 = first_50$`(Intercept)`[i],
      beta_edu = first_50$education_num[i],
      beta_sex = first_50$`factor(sex)1`[i],
      beta_race1 = first_50$`factor(race)1`[i],
      beta_race2 = first_50$`factor(race)2`[i],
      beta_race3 = first_50$`factor(race)3`[i],
      beta_race4 = first_50$`factor(race)4`[i],
      beta_age = first_50$age[i],
      education_num = education_num,
      sex = `factor(sex)1`,  
      race = case_when(
        `factor(race)1` == 1 ~ 1,
        `factor(race)2` == 1 ~ 2,
        `factor(race)3` == 1 ~ 3,
        `factor(race)4` == 1 ~ 4,
        TRUE ~ 0 
      ),
      age = age
    )
  })))


ggplot(predictions, aes(x = id, y = prob)) +
  geom_point(alpha = 0.5, color = "lightblue") +
  labs(y = "Probability of Income > $50K", x = "Individual Index", 
       title = "Predicted Probabilities from First 50 Posterior Samples") +
  theme_minimal()

logodds_prediction <- posterior_linpred(
  income_model_1, newdata = data.frame(education_num = 5)
)

prob_prediction <- posterior_epred(
  income_model_1, newdata = data.frame(education_num = 5)
)

binary_prediction <- posterior_predict(
  income_model_1, newdata = data.frame(education_num = 5)
)

income_model_1_df %>%
  mutate(log_odds = c(logodds_prediction), prob=c(prob_prediction), Y= c(binary_prediction)) %>%
  head(3)

table(binary_prediction)
colMeans(binary_prediction)

```

```{r test}
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(adult_income), size = 0.8 * nrow(adult_income))

train_data <- adult_income[train_indices, ]
test_data <- adult_income[-train_indices, ]

bayesian_model <- stan_glm(
  income ~ education_num + factor(sex) + factor(race) + age,
  data = train_data,
  family = binomial,
  prior = normal(c(0.1, 0.5, -0.5, -0.5, -0.5, -0.5, 0.02), c(0.1, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1)),
  prior_aux = exponential(1),
  chains = 4,
  iter = 5000*2,
  seed = 84735
)

```

```{r prior_summary}
prior_summary(bayesian_model)
summary(bayesian_model)
mcmc_trace(bayesian_model, size = 0.1)
mcmc_dens_overlay(bayesian_model)
```


```{r prediction_probability_training}
bayesian_model_df <- as.data.frame(bayesian_model)
first_50 <- head(bayesian_model_df, 50)

# Function that calculates model trend on probability scale
prob_trend <- function(beta0, beta_edu, beta_sex, beta_race1, beta_race2, beta_race3, beta_race4, beta_age, education_num, sex, race, age) {
  logit <- beta0 + 
    beta_edu * education_num + 
    beta_sex * sex + 
    beta_race1 * (race == 1) + 
    beta_race2 * (race == 2) +
    beta_race3 * (race == 3) +
    beta_race4 * (race == 4) +
    beta_age * age
  
  exp(logit) / (1 + exp(logit))
}



```
